1.Create a Hadoop cluster.

Hadoop
  Hadoop is an open source framework based on Java that manages the storage and processing of large amounts of data for applications. 
  Hadoop uses distributed storage and parallel processing to handle big data and analytics jobs, breaking workloads down into smaller 
  workloads that can be run at the same time.

Four modules comprise the primary Hadoop framework and work collectively to form the Hadoop ecosystem:

•Hadoop Distributed File System (HDFS): HDFS is a distributed file system in which individual Hadoop nodes operate on data 
        that resides in their local storage. This removes network latency, providing high-throughput access to application data.
        It is designed to store large files by splitting them into smaller blocks (typically 128 MB or 256 MB) 
        and distributing them across multiple nodes in the cluster.

•Yet Another Resource Negotiator (YARN): YARN is a resource-management platform responsible for managing compute resources in clusters 
        and using them to schedule users’ applications. It performs scheduling and resource allocation across the Hadoop system.

•MapReduce: MapReduce is a programming model for large-scale data processing. It is used for processing large data sets 
        in a distributed manner. MapReduce works in two phases: the Map phase (where data is processed in parallel) 
        and the Reduce phase (where results are aggregated).

•Hadoop Common: Hadoop Common includes the libraries and utilities used and shared by other Hadoop modules. 

Installation:

•Install JDK on Ubuntu: 
vaagdevi:~$ sudo apt update
 ----------------
|***screenshot***|
 ----------------
vaagdevi:~$ sudo apt install openjdk-8-jdk -y
vaagdevi:~$ java -version

•Install OpenSSH on Ubuntu
vaagdevi:~$ sudo apt install openssh-server openssh-client -y


•Create Hadoop User
vaagdevi:~$ sudo adduser hdoop
vaagdevi:~$ su - hdoop

•Enable Passwordless SSH for Hadoop User
vaagdevi:~$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
vaagdevi:~$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
vaagdevi:~$ chmod 0600 ~/.ssh/authorized_keys
vaagdevi:~$ ssh localhost

•Download and Install Hadoop on Ubuntu
1. Visit the official Apache Hadoop project page and select the version of Hadoop you want to implement. select binary file.
2. Use the provided mirror link and download the Hadoop package using the wget command:
vaagdevi:~$ wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz
3. Once the download completes, use the tar command to extract the .tar.gz file and initiate the Hadoop installation:
vaagdevi:~$ tar xzf hadoop-3.4.0.tar.gz

•Configure Hadoop Environment Variables (bashrc)
The .bashrc config file is a shell script that initializes user-specific settings, such as environment variables, aliases, and functions.
1. Edit the .bashrc shell configuration file using a text editor of your choice (we will use nano):
vaagdevi:~$ nano .bashrc
2. Define the Hadoop environment variables by adding the following content to the end of the file:
#Hadoop Related Options
export HADOOP_HOME=/home/hdoop/hadoop-3.4.0
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
3. Once you add the variables, save and exit the .bashrc file.
4. Run the command below to apply the changes to the current running environment:
vaagdevi:~$ source ~/.bashrc
•Edit hadoop-env.sh File
1. Use the previously created $HADOOP_HOME variable to access the hadoop-env.sh file:
vaagdevi:~$ nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
2. Uncomment the $JAVA_HOME variable and replace the following.
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64


After editing the .bashrc file, source it to apply the changes:
vaagdevi: /opt/hadoop $ source ~/.bashrc
Step 4: Configure Hadoop
Navigate to the Hadoop configuration directory and edit the configuration files.
1. Edit core-site.xml
Configure the default file system (HDFS):
vaagdevi: /opt/hadoop $ cd /etc/hadoop
vaagdevi: /opt/hadoop/etc/hadoop $ nano core-site.xml
Add the following configuration inside <configuration>:
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://localhost:9000</value>
</property>

2. Edit hdfs-site.xml
Configure HDFS settings:
vaagdevi: /opt/hadoop/etc/hadoop $ nano hdfs-site.xml
Add the following configuration inside <configuration>:
<property>
  <name>dfs.replication</name>
  <value>1</value> <!-- Single-node setup, replication = 1 -->
</property>

3. Edit mapred-site.xml
Configure the MapReduce framework:
vaagdevi: /opt/hadoop/etc/hadoop $ cp mapred-site.xml.template mapred-site.xml
vaagdevi: /opt/hadoop/etc/hadoop $ nano mapred-site.xml
Add the following inside <configuration>:
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

4. Edit yarn-site.xml
Configure YARN settings:
vaagdevi: /opt/hadoop/etc/hadoop $ nano yarn-site.xml
Add the following configuration inside <configuration>:
<property>
  <name>yarn.resourcemanager.address</name>
  <value>localhost:8032</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.nodemanager.local-dirs</name>
  <value>/opt/hadoop/yarn/local</value>
</property>



Step 5: Format the Hadoop File System
Before starting the Hadoop daemons, you need to format the HDFS filesystem:
vaagdevi: /opt/hadoop/sbin $ hdfs namenode -format
Step 6: Start Hadoop Services
Start the Hadoop daemons (NameNode, DataNode, ResourceManager, NodeManager, etc.):
# Start HDFS daemons
vaagdevi: /opt/hadoop/sbin $ start-dfs.sh
# Start YARN daemons
vaagdevi: /opt/hadoop/sbin $ start-yarn.sh
Step 7: Verify the Installation
Check if the Hadoop daemons are running:
vaagdevi: /opt/hadoop/sbin $ jps
You should see the following processes running:
NameNode
DataNode
ResourceManager
NodeManager

Step 8: Access the Hadoop Web UI
HDFS UI: Open http://localhost:9870/ to access the HDFS UI.
YARN UI: Open http://localhost:8088/ to access the YARN UI.
After editing the .bashrc file, source it to apply the changes:
vaagdevi: /opt/hadoop $ source ~/.bashrc
Step 4: Configure Hadoop
Navigate to the Hadoop configuration directory and edit the configuration files.
1. Edit core-site.xml
Configure the default file system (HDFS):
vaagdevi: /opt/hadoop $ cd /etc/hadoop
vaagdevi: /opt/hadoop/etc/hadoop $ nano core-site.xml
Add the following configuration inside <configuration>:
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://localhost:9000</value>
</property>

2. Edit hdfs-site.xml
Configure HDFS settings:
vaagdevi: /opt/hadoop/etc/hadoop $ nano hdfs-site.xml
Add the following configuration inside <configuration>:
<property>
  <name>dfs.replication</name>
  <value>1</value> <!-- Single-node setup, replication = 1 -->
</property>

3. Edit mapred-site.xml
Configure the MapReduce framework:
vaagdevi: /opt/hadoop/etc/hadoop $ cp mapred-site.xml.template mapred-site.xml
vaagdevi: /opt/hadoop/etc/hadoop $ nano mapred-site.xml
Add the following inside <configuration>:
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

4. Edit yarn-site.xml
Configure YARN settings:
vaagdevi: /opt/hadoop/etc/hadoop $ nano yarn-site.xml
Add the following configuration inside <configuration>:
<property>
  <name>yarn.resourcemanager.address</name>
  <value>localhost:8032</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.nodemanager.local-dirs</name>
  <value>/opt/hadoop/yarn/local</value>
</property>



Step 5: Format the Hadoop File System
Before starting the Hadoop daemons, you need to format the HDFS filesystem:
vaagdevi: /opt/hadoop/sbin $ hdfs namenode -format
Step 6: Start Hadoop Services
Start the Hadoop daemons (NameNode, DataNode, ResourceManager, NodeManager, etc.):
# Start HDFS daemons
vaagdevi: /opt/hadoop/sbin $ start-dfs.sh
# Start YARN daemons
vaagdevi: /opt/hadoop/sbin $ start-yarn.sh
Step 7: Verify the Installation
Check if the Hadoop daemons are running:
vaagdevi: /opt/hadoop/sbin $ jps
You should see the following processes running:
NameNode
DataNode
ResourceManager
NodeManager

Step 8: Access the Hadoop Web UI
HDFS UI: Open http://localhost:9870/ to access the HDFS UI.
YARN UI: Open http://localhost:8088/ to access the YARN UI.

