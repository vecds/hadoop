2.Implement a simple mapreduce job that builds an inverted index on the set of input documents(Hadoop).

A MapReduce is a data processing tool which is used to process the data parallelly in a distributed form.The MapReduce is a paradigm which has two phases,
a "map" phase that transforms input data and a "reduce" phase that aggregates the transformed data. 

1. Map Phase:
Purpose:
The map phase takes a large dataset and processes it in parallel across multiple nodes or processors. 
Function:
Each node processes a portion of the data, applying a specific function (the "map" function) to transform the data into key-value pairs. 
Example:
The map phase could involve splitting the file into smaller chunks, and for each chunk, the map function could extract each word and pair it with a count of 1 
(e.g., ("word1", 1), ("word2", 1), etc.). 

2. Reduce Phase:
Purpose:
The reduce phase takes the key-value pairs generated by the map phase and aggregates them to produce a final output. 
Function:
The "reduce" function processes the key-value pairs, combining the values associated with the same key to produce a summarized output. 
Example:
the reduce phase would receive the key-value pairs generated by the map phase (e.g., ("word1", 1), ("word1", 1), ("word2", 1), etc.).
It would then aggregate the values associated with the same key (e.g., "word1") by summing them up to produce the final output (e.g., ("word1", 2), ("word2", 1), etc.). 

Inverted index:
An inverted index is a data structure used primarily in text search engines. It maps terms (words) to their locations in a set of documents. 
This type of index allows for fast full-text searches, making it ideal for applications like search engines and document retrieval systems.

Implementation in Hadoop
# add these line to the hadoop-env.sh
export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar

#Change  <value>yarn</value> 	 to		<value>local</value> 	
in mapred-site.xml

# Start hadoop dfs deamons: 
vaagdevi:~/hdoop$ start-dfs.sh

# create a java program
vaagdevi:~/hdoop$ nano invertedindex.java


import java.io.IOException;
import java.util.HashSet;
import java.util.Set;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class invertedindex {

    // Mapper Class

    public static class invertedindexMapper
            extends Mapper<LongWritable, Text, Text, Text> {

        private Text word = new Text();
        private Text fileName = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {

            // Get file name
            String file = ((FileSplit) context.getInputSplit())
                    .getPath().getName();
            fileName.set(file);

            // Tokenize line
            String[] tokens = value.toString().toLowerCase().split("\\W+");

            for (String token : tokens) {
                if (!token.isEmpty()) {
                    word.set(token);
                    context.write(word, fileName);
                }
            }
        }
    }

    // Reducer Class

    public static class invertedindexReducer
            extends Reducer<Text, Text, Text, Text> {

        private Text result = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context)
                throws IOException, InterruptedException {

            Set<String> files = new HashSet<>();

            for (Text val : values) {
                files.add(val.toString());
            }

            result.set(String.join(",", files));
            context.write(key, result);
        }
    }

    // Driver (Main)

    public static void main(String[] args) throws Exception {

        if (args.length != 2) {
            System.err.println("Usage: InvertedIndex <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Inverted Index");

        job.setJarByClass(invertedindex.class);
        job.setMapperClass(invertedindexMapper.class);
        job.setReducerClass(invertedindexReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}



#Create jar file

Run the following commands to compile invertedindex.java and create a jar file.

vaagdevi:~/hdoop/hadoop$ bin/hadoop com.sun.tools.javac.Main /home/vaagdevi/hdoop/invertedindex.java

vaagdevi:~/hdoop$ jar cf invertedindex.jar invertedindex*.class

# Run the jar file

Create 2 documents ( h1.txt, h2.txt )  and copy in hdfs /input directory and run then the following command

vaagdevi:~/hdoop$ hadoop jar /home/vaagdevi/hdoop/invertedindex.jar invertedindex /input /output

To check the Inverted Index  
vaagdevi:~/hdoop$ hadoop dfs -cat /output/part-r-00000
