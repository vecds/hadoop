2.Implement a simple mapreduce job that builds an inverted index on the set of input documents(Hadoop).

MapReduce is a computational paradigm used for processing and generating large datasets by dividing the work into two phases: 
a "map" phase that transforms input data and a "reduce" phase that aggregates the transformed data. 

1. Map Phase:
Purpose:
The map phase takes a large dataset and processes it in parallel across multiple nodes or processors. 
Function:
Each node processes a portion of the data, applying a specific function (the "map" function) to transform the data into key-value pairs. 
Example:
The map phase could involve splitting the file into smaller chunks, and for each chunk, the map function could extract each word and pair it with a count of 1 
(e.g., ("word1", 1), ("word2", 1), etc.). 

2. Reduce Phase:
Purpose:
The reduce phase takes the key-value pairs generated by the map phase and aggregates them to produce a final output. 
Function:
The "reduce" function processes the key-value pairs, combining the values associated with the same key to produce a summarized output. 
Example:
the reduce phase would receive the key-value pairs generated by the map phase (e.g., ("word1", 1), ("word1", 1), ("word2", 1), etc.).
It would then aggregate the values associated with the same key (e.g., "word1") by summing them up to produce the final output (e.g., ("word1", 2), ("word2", 1), etc.). 

inverted index:
An inverted index is a data structure used primarily in text search engines. It maps terms (words) to their locations in a set of documents. 
This type of index allows for fast full-text searches, making it ideal for applications like search engines and document retrieval systems.

Implementation in Hadoop
# add these line to the hadoop-env.sh
export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar

Start hadoop dfs deamons: 
vaagdevi:~/hdoop$ start-dfs.sh

# create a java program
vaagdevi:~/hdoop$ nano invertedindex.java

import java.util.StringTokenizer;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class invertedindex {



  public static class TokenizerMapper extends Mapper<Object, Text, Text, Text>
{


    private Text word = new Text();
    Text docID = new Text();

    public void map(Object key, Text value, Context context)
    		throws IOException, InterruptedException {

     	     String line = value.toString();
	    StringTokenizer tk = new StringTokenizer(line);
	     String docIDstring = tk.nextToken();
	    docID = new Text(docIDstring);
	     //get the document ID first from the first token

	    //clean the input data  : 1.remove punctuations 2.remove digits 3.convert to lower case
             line = line.replaceAll("\\p{Punct}"," ");
	     line = line.replaceAll("\\d"," ");
	     line = line.toLowerCase();

  	     StringTokenizer itr = new StringTokenizer(line);

	       while (itr.hasMoreTokens()) {
			 word.set(itr.nextToken());
	         	 context.write(word, docID);
	       }
    }
  }

  public static class IntSumReducer extends Reducer<Text,Text,Text,Text> {


    public void reduce(Text keyword, Iterable<Text> docIDList, Context output) 
    		throws IOException, InterruptedException {

    	HashMap<String,Integer> hashtable = new HashMap<String,Integer>();
    	  Iterator<Text> itr = docIDList.iterator();
    	   int count = 0;
    	   String docID = new String();

    	   while (itr.hasNext()) {
    		   docID = itr.next().toString();

		    if(hashtable.containsKey(docID)){
		    	     count = (hashtable.get(docID));
		    	     count += 1;
		    	     hashtable.put(docID, count);
	    	    }else{
		   	hashtable.put(docID, 1);
		    }
    	   }

    	   StringBuffer buf = new StringBuffer("");
	   for(Map.Entry<String, Integer> h: hashtable.entrySet())
		buf.append(h.getKey() + ":" + h.getValue() + "\t");


	   Text optext = new Text(buf.toString());
    	   output.write(keyword, optext);

    }
  }

  public static void main(String[] args) throws Exception {
	    Configuration conf = new Configuration();
	    Job job = Job.getInstance(conf, "Inverted Index");

	    job.setJarByClass(invertedindex.class);
	    job.setMapperClass(TokenizerMapper.class);
	    job.setReducerClass(IntSumReducer.class);
	    job.setOutputKeyClass(Text.class);
	    job.setOutputValueClass(Text.class);
	    FileInputFormat.addInputPath(job, new Path(args[0]));
	    FileOutputFormat.setOutputPath(job, new Path(args[1]));

	    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}


#Create jar file

Run the following commands to compile InvertedIndex.java and create a jar file.

vaagdevi:~/hdoop/hadoop$ bin/hadoop com.sun.tools.javac.Main /home/navi/hdoop/index.java

vaagdevi:~/hdoop$ jar cf index.jar index*.class

# Run the jar file

Create 2 documents and copy in hdfs /input directory and run then the following command

vaagdevi:~/hdoop$ hadoop jar /home/navi/hdoop/index.jar index /input /output

vaagdevi:~/hdoop$ hadoop dfs -cat /output1/part-r-00000
