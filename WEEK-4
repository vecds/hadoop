4. Store and retrieve data in pig.

Apache Pig is a high-level platform for processing large data sets using Hadoop. 
Hadoop stores raw data coming from various sources like IOT, 
websites, mobile phones, etc. and preprocessing is done in Map-reduce. 
Pig framework converts any pig job into Map-reduce hence we can use the pig to do the ETL 
  (Extract Transform and Load) process on the raw data. 

â€¢	Developed by Yahoo!
â€¢	Part of the Hadoop ecosystem.
â€¢	Uses a scripting language called Pig Latin.

Key Features
â€¢	High-level language (Pig Latin) for data analysis.
â€¢	Extensible: You can write your own functions (UDFs) in Java, Python, etc..
â€¢	Handles complex data like nested structures.
â€¢	Optimized: Pig scripts are automatically converted into optimized MapReduce jobs.

Pig Latin Overview
Pig Latin is a data flow language. It uses a step-by-step approach where each step in the script defines a transformation.
Apache pig framework has below major components as part of its Architecture:
1.	Pig Latin Script
2.	Grunt Shell (Interactive shell)
3.	Pig Engine (Executes the script)
4.	Parser(checks syntax, output in DAG)
5.	Optimizer(perform logical optimization)
6.	Compiler(generates a series of Map-Reduce jobs)
7.	Execution Modes: (submitted to Hadoop in sorted order)
  o	Local Mode: Runs on a single machine (good for testing)
  o	MapReduce Mode: Runs on Hadoop cluster
 
Use Cases
â€¢	ETL (Extract, Transform, Load) operations.
â€¢	Log processing.
â€¢	Data preparation for machine learning.
â€¢	Ad-hoc data analysis.

Advantages of Pig
â€¢	Easier to write and understand than raw MapReduce.
â€¢	Faster development time.
â€¢	Scales well with Hadoop.
â€¢	Good for schema-less, semi-structured data.

Limitations
â€¢	Not suitable for real-time processing.
â€¢	Less popular now due to rise of Apache Spark.
â€¢	Requires Hadoop infrastructure.

Apache Pig Installation (Local Mode)
Step 1: Prerequisites
Before installing Pig, make sure you have the following installed:
1.	Java (JDK) â€“ version 8 is recommended.
2.	Hadoop (optional for local mode, but useful).
3.	Pig binary package.

Step 2: ðŸ“¦ Download Apache Pig
Get the latest stable Pig release from the official site:
wget https://downloads.apache.org/pig/pig-0.16.0/pig-0.16.0.tar.gz

Step 3: ðŸ“‚ Extract and Set Up Pig
Extract the downloaded archive:
tar -xvzf pig-0.16.0.tar.gz
mv pig-0.16.0 pig

Add Pig to your environment variables:

export PIG_HOME=~/pig
export PATH=$PIG_HOME/bin:$PATH

Step 4:  Run Pig in Local Mode
You can now launch Pig in local mode (doesn't need Hadoop cluster):
pig -x local

Youâ€™ll enter the interactive Grunt shell:

grunt>

sample commands:

create a csv file-  employees.csv

1,John,25,50000,IT
2,Alice,30,60000,HR
3,Bob,28,55000,IT
4,Carol,35,70000,Finance
5,David,40,80000,HR


1. DUMP
grunt> employees = LOAD '/home/vaagdevi/hdoop/employees.csv' USING PigStorage(',') 
        AS (id:int, name:chararray, age:int, salary:int, dept:chararray);
grunt> DUMP employees;

2. FILTER
grunt> high_salary = FILTER employees BY salary > 60000;
grunt> DUMP high_salary;


3. FOREACH GENERATE
grunt> names_salary = FOREACH employees GENERATE name, salary;
grunt> DUMP names_salary;

4. GROUP + COUNT
grunt> group_by_dept = GROUP employees BY dept;

grunt> dept_count = FOREACH group_by_dept
             GENERATE group AS dept,
             COUNT(employees) AS total_employees;

grunt> DUMP dept_count;

5. ORDER BY

grunt> sorted_emp = ORDER employees BY salary DESC;
grunt> DUMP sorted_emp;


Step 5: STORE AND RETRIVE DATA 

To test, create a simple Pig Latin script (e.g., script.pig):

Create a script and move into hdfs

$ script.pig

data = LOAD '/input/employees.csv' USING PigStorage(',') AS (id:int, name:chararray, age:int, salary:int, dept:chararray);
filtered = FILTER data BY age > 25;
DUMP filtered;
STORE filtered INTO '/output/filtered_employees' USING PigStorage(',');

*save and exit file. ctrl+s & ctrl+x.

vaagdevi:~/hdoop$ hdfs dfs -mkdir /input
vaagdevi:~/hdoop$ hdfs dfs -mkdir /output
vaagdevi:~/hdoop$ hdfs dfs -put /home/vaagdevi/hdoop/employees.csv /input/
vaagdevi:~/hdoop$ hdfs dfs -ls /input/
vaagdevi:~/hdoop$ pig script.pig
vaagdevi:~/hdoop$ hdfs dfs -cat /output/filtered_employees/part-m-00000


!!!Basic Pig Commands

1. Fs: This will list all the files in the HDFS

grunt> fs â€“ls

2. Clear: This will clear the interactive Grunt shell.

grunt> clear

3. History: This command shows the commands executed so far.

grunt> history

4. Reading Data: Assuming the data resides in HDFS, we need to read data to Pig.

grunt> college_students = LOAD â€˜hdfs://localhost:9000/pig_data/college_data.txtâ€™

USING PigStorage(â€˜,â€™) as ( id: int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );

PigStorage() is the function that loads and stores data as structured text files.

5. Storing Data: The store operator stores the processed/loaded data.

grunt> STORE college_students INTO â€˜ hdfs://localhost:9000/pig_Output/ â€˜ USING PigStorage (â€˜,â€™);

Here, â€œ/pig_Output/â€ is the directory where the relation needs to be stored.

6. Dump Operator: This command displays the results on the screen. It usually helps in debugging.

grunt> Dump college_students;

7. Describe Operator: It helps the programmer view the relationâ€™s schema.

grunt> describe college_students;

8. Explain: This command helps to review the logical, physical, and map-reduce execution plans.

grunt> explain college_students;

